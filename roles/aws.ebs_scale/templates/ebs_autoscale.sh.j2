#!/bin/bash

# Allows to attach and remove EBS volumes managed under LVM to
# have a dynamically sized partition attached to an EC2 instance
#
# Intance needs either to be launched with a role able to access to relevant AWS API endpoints
# or the credentials can be hardcoded in the config.
#
# Minimal IAM Role:
# {
#  "Version": "2012-10-17",
#  "Statement": [
#    {
#      "Sid": "EBS-autoscale",
#      "Effect": "Allow",
#      "Action": [
#        "ec2:AttachVolume",
#        "ec2:CreateVolume",
#        "ec2:DeleteVolume",
#        "ec2:CreateTags",
#        "ec2:DeleteTags",
#        "ec2:DescribeInstanceAttribute",
#        "ec2:DescribeInstances",
#        "ec2:DescribeVolumeAttribute",
#        "ec2:DescribeVolumeStatus",
#        "ec2:DescribeVolumes",
#        "ec2:DetachVolume",
#        "ec2:EnableVolumeIO",
#        "ec2:ModifyInstanceAttribute",
#        "ec2:ModifyVolumeAttribute"
#      ],
#      "Resource": [
#        "*"
#      ]
#    }
#  ]
# }
#
#
#
#Print all the things.
set -x

# Base name for VG and LV.
# VG = vg_$NAME
# LV = lv_$NAME
NAME="{{vg_name}}"

# Where to mount it
MOUNT_POINT="{{vg_mount_point}}"

# Size of each disk in GB
# Maximum EBS size is 1024GB
DISK_SIZE="{{vg_disk_size}}"

# %age of free space (relative to disk size) before adding a new disk
SPACE_UP="{{vg_space_up}}"

# %age of free space above 1 free disk (relative to disk size) before removing one.
SPACE_DOWN="{{vg_space_down}}"

# If you want to start with a higher disk identifier to leave room for other partitions.
# /dev/sda is the root device
# /dev/sdb is the default instance-store partition
# No disk above /dev/sdz will be created

START_DISK_LETTER='{{vg_start_disk_letter}}'

# Minimum number of disks to keep
MIN_DISKS="{{vg_min_disks}}"

# Maximum number of disks to use
MAX_DISKS="{{vg_max_disks}}"

# base AWS CLI command
AWS_EC2="/usr/local/bin/aws ec2"

# NSQ Address
# e.g. demo-services.aptrust.org:4151
NSQ_ADDRESS="{{nsq_address}}"

NOTIFICATION_TYPE='slack'

function send_msg {
    if [[ $NOTIFICATION_TYPE == 'slack' ]];then
      send_slack $*
    else
      send_email $*
    fi
  }

function log {
    echo "[$(date --rfc-3339=seconds)]: $*"
}

send_slack() {
    EBS_SLACK_WEBHOOK="{{ebs_slack_webhook}}"
    EBS_SLACK_CHANNEL="{{ebs_slack_channel}}"
    EBS_SLACK_USERNAME=${EBS_SLACK_USERNAME:-$(hostname | cut --delimiter=. --fields=1)}
    EBS_SLACK_ICON_EMOJI=${EBS_SLACK_ICON_EMOJI:-:slack:}
		EBS_SLACK_MESSAGE=$*

		curl --silent --data-urlencode \
			"$(printf 'payload={"text": "%s", "channel": "%s", "username": "%s",
            "as_user": "true", "link_names": "true", "icon_emoji": "%s" }' \
					"${EBS_SLACK_MESSAGE}" \
					"${EBS_SLACK_CHANNEL}" \
					"${EBS_SLACK_USERNAME}" \
					"${EBS_SLACK_ICON_EMOJI}" \
			)"	${EBS_SLACK_WEBHOOK} || true

}

send_email() {
		SENDER=$(whoami)
		RECEIVER="christian@aptrust.org"
		USER="noreply"

		SUBJECT=$1
		BODY=${2:-''}

		MAIL_TXT="Subject: $SUBJECT\nFrom: $SENDER\nTo: $RECEIVER\n\n$BODY"
		echo -e $MAIL_TXT | sendmail -t
}


chr() {
  # converts decimal value to its ASCII character representation
  printf \\$(printf '%03o' $1)
}
ord() {
  # converts ASCII character to its decimal value
  printf '%d' "'$1"
}


check_if_mounted(){
    log ${FUNCNAME[0]}
    if grep -qs "${MOUNTPOINT}" /proc/mounts; then
        log "${MOUNTPOINT} is mounted. All good."
    else
        log "${MOUNTPOINT} not mounted. Go panic."
        # TODO: Alert email.
        exit 1
    fi
}

add_to_fstab(){
    log ${FUNCNAME[0]}
    fstab=/etc/fstab
    # Getting the device name since Ubuntu thinks its funny to double
    # dash the vg.
    MOUNT_DEVICE=`grep $MOUNT_POINT /proc/mounts | awk '{print $1}'`
    fstab_entry="$MOUNT_DEVICE $MOUNT_POINT ext4 rw,nobootwait 0 0"

    if $(grep -q "$fstab_entry" $fstab); then
         log "Entry in fstab exists. Doing nothing."
    else
        log "Entry in fstab does not exists. Adding for persistence."
        sudo cp $fstab /etc/fstab.bak
        sudo echo $fstab_entry >> /etc/fstab
        if sudo mount -fav; then
          log "Checked fstab, looks good."
        else
          log "Error in fstab entry. Need manual intervention.\
                Reverting to previous version."
          sudo cp /etc/fstab.bak $fstab
          send_msg "fstab entry has error on $HOSTNAME"
          exit 1
        fi
    fi
  }

next_disk() {
    log ${FUNCNAME[0]}
# TODO: This logic doesn't work. Redo.
    # No disk exist yet
    if [ "x"$1 == "x" ]; then
        DISK_LETTER=$START_DISK_LETTER
    else
        num=`ord $1`
# TODO: Allow for second-level disk letters, https://www.pivotaltracker.com/story/show/154039718
        if [ $num -ge 122 ];then
            # Too lazy to handle /dev/sdaa, 24 disks (24TB) should be enough, no?
            log "No more disk letter available"
            send_msg "Ran out of more disk letters." "Need manual intervention"
            DISK_LETTER=""
            return 1
        fi
        let num=$num+1
        DISK_LETTER=`chr $num`
    fi
}

add_disk() {
    log "${FUNCNAME[0]}" "$1"
# TODO: This condition does not bear the correct result. Need to fix.
    if [ "x"$1 == "x" ]; then
        ADISK_LETTER=$START_DISK_LETTER
    else
	      ADISK_LETTER=$1
    fi
    printf 'Adding disk letter: %s\n' $ADISK_LETTER

    $AWS_EC2 create-volume --volume-type gp2 --size $DISK_SIZE --availability-zone $AV_ZONE --tag-specifications "ResourceType=volume,Tags=[{Key=Name,Value=ebs_scale_${NAME}_disk_${ADISK_LETTER}}]" > /etc/ebs_volume_info
    volume_id=`jq -r '.VolumeId' /etc/ebs_volume_info`

    volume_status=""
    while [ "x"$volume_status != "xavailable" ]; do
        sleep 1
        $AWS_EC2 describe-volumes --volume-ids $volume_id > /etc/ebs_volume_info
        volume_status=`jq -r '.Volumes[].State' /etc/ebs_volume_info`
    done
    last_disk
    next_disk $LAST_DISK_LETTER || return 1

    $AWS_EC2 attach-volume --volume-id $volume_id --instance-id $INSTANCE_ID --device /dev/sd${DISK_LETTER} || return 1
    attached=""
    while [ "x"$attached != "xattached" ]; do
        sleep 1
        $AWS_EC2 describe-instances --instance-ids $INSTANCE_ID > /etc/ec2_instance_info
        attached=`jq --arg drive "/dev/sd${DISK_LETTER}" -r '.Reservations[].Instances[].BlockDeviceMappings[] | {name: .DeviceName, status: .Ebs.Status}|select(.name==$drive)|.status' /etc/ec2_instance_info`
    done
    $AWS_EC2 modify-instance-attribute --instance-id $INSTANCE_ID --block-device-mappings "[{\"DeviceName\": \"/dev/sd${DISK_LETTER}\",\"Ebs\":{\"DeleteOnTermination\":true}}]" || return 1

    sudo pvcreate /dev/xvd${DISK_LETTER} || return 1 #/dev/sdX is attached as /dev/xvdX
    add_to_fstab
    send_msg "EBS Scale /dev/xvd${DISK_LETTER} on server $HOSTNAME" \
               "EBS scale extended LVM drive by $DISK_SIZE\n `df -h |grep $MOUNT_POINT`"
}

last_disk() {
    log "${FUNCNAME[0]}"
    $AWS_EC2 describe-instances --instance-ids $INSTANCE_ID > /etc/ec2_instance_info
    root_device=`jq -r '.Reservations[].Instances[].RootDeviceName' /etc/ec2_instance_info`
    LAST_DISK_LETTER=`jq -r '.Reservations[].Instances[].BlockDeviceMappings[].DeviceName' /etc/ec2_instance_info | grep -v $root_device | sort | tail -n1 | sed -e 's/^.*\([a-z]\)$/\1/'`
}

initialize_lv() {
    log "${FUNCNAME[0]}"
    $AWS_EC2 describe-instances --instance-ids $INSTANCE_ID > /etc/ec2_instance_info
    sudo vgcreate vg_$NAME /dev/xvd${DISK_LETTER}
    sudo lvcreate -l 100%VG -n lv_$NAME vg_$NAME
    sudo mkfs.ext4 /dev/vg_$NAME/lv_$NAME

    sudo mkdir -p $MOUNT_POINT
    sudo mount /dev/vg_$NAME/lv_$NAME $MOUNT_POINT

    add_to_fstab

}

extend_lv() {
    log "${FUNCNAME[0]}"
    sudo vgextend vg_$NAME /dev/xvd${DISK_LETTER}
    sudo lvextend -l +100%FREE /dev/vg_$NAME/lv_$NAME
    sudo resize2fs /dev/vg_$NAME/lv_$NAME
}

INSTANCE_ID=`wget -q -O - http://169.254.169.254/latest/meta-data/instance-id`
AV_ZONE=`wget -q -O - http://169.254.169.254/latest/meta-data/placement/availability-zone`
REGION="`echo \"$AV_ZONE\" | sed -e 's:\([0-9][0-9]*\)[a-z]*\$:\\1:'`"

# Create base VG if not existing
if [[ ! -d "/dev/vg_$NAME/lv_$NAME" && ! -L "/dev/vg_$NAME/lv_$NAME" ]]; then
    if [ ! `which unzip` ];then
        apt-get -y update
        apt-get -y install unzip lvm2 jq
    fi
    if [ ! -f /usr/local/bin/aws ]; then
        wget -O awscli-bundle.zip https://s3.amazonaws.com/aws-cli/awscli-bundle.zip
        unzip -u awscli-bundle.zip
        ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws
        mkdir -p ~/.aws
        # If we want to hardcode the credentials in the config:
        # cat > ~/.aws/config << EOF
# [default]
# aws_access_key_id = MY_ACCESS_KEY
# aws_secret_access_key = MY_SECRET
# region = $REGION
# EOF

        cat > ~/.aws/config << EOF
[default]
region = $REGION
EOF
    fi
# Main function
    add_disk || exit 1
    initialize_lv || exit 1
    for n in $(seq 2 $MIN_DISKS);do
        add_disk || exit 1
        extend_lv || exit 1
    done
    check_if_mounted || exit 1
fi

# Checking if we need to do something
# TODO: Add a conditional that checks if mount point exists,if it does not we need to mount it.
free_space=`df | grep $MOUNT_POINT | awk '{print $4}'`

# Calc threshold values
min_free_space=`echo "($SPACE_UP/100*$DISK_SIZE)*1000000" | bc -l`
max_free_space=`echo "(($SPACE_DOWN+100)/100*$DISK_SIZE)*1000000" | bc -l`

# Check if more space or less space is needed
more_space_needed=`echo $free_space'<'$min_free_space | bc -l`
less_space_needed=`echo $free_space'>'$max_free_space | bc -l`

# Amount of mounted disks
actual_disks=`sudo lvdisplay /dev/vg_${NAME}/lv_${NAME}|grep Segments|awk '{print $2}'`

# Check if we are inside disk limit bounds
max_limit_reached=`echo $actual_disks'>='$MAX_DISKS | bc -l`
min_limit_reached=`echo $actual_disks'<='$MIN_DISKS | bc -l`

# When more space needed, add and extend as long we are insinde the disk amount limit
if [[ $more_space_needed -eq 1 ]]; then
    if [[ $max_limit_reached -eq 1 ]]; then
        echo "Space needed but maximum disk limit reached"
    else
        add_disk || exit 1
        extend_lv || exit 1
    fi
fi

check_nsq_inflight(){
    # If NSQ is not used it will default to empty and will default to the else clause below.
     airtraffic=`curl http://$NSQ_ADDRESS/stats | grep channel|awk '{ print $7,$8; }'| grep -o '[0-9]'|tr -d "\n"`
     if [ "$airtraffic" -gt "0" ]; then
       echo "Seeing some air traffic: $airtraffic Please stand by. "
       nsq_result="standby"
     else
       echo "Nobody is flying. Good to go. Shutting down services"
       nsq_result="takeoff"
     fi
}

# When less space needed and still inside the disk limit print out manual action to carry out.
# resize2fs can't do a resize while the parition is mounted. The partition needs to be unmounted first,
# which mostly requires the service to be put in maintencance mode

if [[ $less_space_needed -eq 1 ]]; then
    if [[ $min_limit_reached -eq 1 ]]; then
        echo "Too much space but minimum disk limit reached"
    else
        echo "$timestamp ---SHRINKING VOLUME START----"
        TOTAL_SPACE=`df | grep $MOUNT_POINT | awk '{print $2}'`
        NEW_SIZE=`echo "($TOTAL_SPACE-($DISK_SIZE*1000000*1.1))/1024" | bc`
        LAST_VOLUME=`jq -r '.Reservations[].Instances[].BlockDeviceMappings[].Ebs.VolumeId' /etc/ec2_instance_info | tail -n1`
        cat << EOF
        # Check for inflight items to avoid stopping services if not necessary.
        # Shrinking will commence once no worker is processing items.
        check_nsq_inflight
        if [ $nsq_result == "takeoff" ];then
            echo "$timestamp - Stopping exchange services"
            sudo supervisorctl stop all
            # Set dummy value to run through loop at least once
            openfiles=100
            while [ $openfiles -gt 0 ]; do
              echo "$timestamp - Waiting until no open files are present"
              sleep 3
              openfiles=`sudo lsof | grep $MOUNT_POINT |wc -l`
              echo "$timestamp - Open files= $openfiles"
            done
            umount $MOUNT_POINT
            if [ $? -eq 0 ]; then
              LAST_VOLUME=`jq -r '.Reservations[].Instances[].BlockDeviceMappings[].Ebs.VolumeId' /etc/ec2_instance_info | tail -n1`
              echo "$timestamp - The LAST_VOLUME is $LAST_VOLUME"
              e2fsck -f -y /dev/vg_${NAME}/lv_${NAME}
              #NOTE: This may be redundant as lvreduce already resizes..::
              # WHY does resize2fs need NEW_SIZE
              resize2fs /dev/vg_${NAME}/lv_${NAME} ${NEW_SIZE}M
              lvreduce -f -r -L -${DISK_SIZE}.1G vg_${NAME}/lv_${NAME}
              vgreduce -a vg_${NAME}
              lvextend -l +100%FREE /dev/vg_${NAME}/lv_${NAME}
              e2fsck -f -y /dev/vg_${NAME}/lv_${NAME}
              resize2fs /dev/vg_${NAME}/lv_${NAME}
              mount /dev/vg_${NAME}/lv_$NAME $MOUNT_POINT
              $AWS_EC2 detach-volume --instance-id $INSTANCE_ID --volume-id $LAST_VOLUME
              volume_status=""
              while [ "x"$volume_status != "xavailable" ]; do
                sleep 1
                $AWS_EC2 describe-volumes --volume-ids $LAST_VOLUME > /etc/ebs_to_shrink
                volume_status=`jq -r '.Volumes[].State' /etc/ebs_to_shrink`
              done
              $AWS_EC2 delete-volume --volume-id $LAST_VOLUME
              # Update ec2_instance_info to have the currrently attached volumes up-to-date.
              $AWS_EC2 describe-instances --instance-ids $INSTANCE_ID > /etc/ec2_instance_info
              sudo supervisorctl start all
              echo "$timestamp ---SHRINKING VOLUME END----"
            else
              send_msg "$timestamp \n Unable to unmount. Aborting shrinking on $hostname"
              exit 1
            fi
        else
            echo "$timestamp -- Can't shrink right now, trying again later"
        fi
EOF
    fi
fi
