#!/bin/bash
# Version: "{{githash}}"
# Date: "{{ansible_date_time.iso8601}}"
# Allows to attach and remove EBS volumes managed under LVM to
# have a dynamically sized partition attached to an EC2 instance
#
# Intance needs either to be launched with a role able to access to relevant AWS API endpoints
# or the credentials can be hardcoded in the config.
#
# Minimal IAM Role:
# {
#  "Version": "2012-10-17",
#  "Statement": [
#    {
#      "Sid": "EBS-autoscale",
#      "Effect": "Allow",
#      "Action": [
#        "ec2:AttachVolume",
#        "ec2:CreateVolume",
#        "ec2:DeleteVolume",
#        "ec2:CreateTags",
#        "ec2:DeleteTags",
#        "ec2:DescribeInstanceAttribute",
#        "ec2:DescribeInstances",
#        "ec2:DescribeVolumeAttribute",
#        "ec2:DescribeVolumeStatus",
#        "ec2:DescribeVolumes",
#        "ec2:DetachVolume",
#        "ec2:EnableVolumeIO",
#        "ec2:ModifyInstanceAttribute",
#        "ec2:ModifyVolumeAttribute"
#      ],
#      "Resource": [
#        "*"
#      ]
#    }
#  ]
# }
#
#
#
#Print all the things.
#set -x
PIDFILE=/var/run/ebs_autoscale.pid
if [ -f $PIDFILE ]
then
  PID=$(cat $PIDFILE)
  $(ps -p $PID > /dev/null 2>&1)
  if [ $? -eq 0 ]
  then
    echo "Process already running"
    exit 1
  else
    ## Process not found assume not running
    echo $$ > $PIDFILE
    if [ $? -ne 0 ]
    then
      echo "Could not create PID file"
      exit 1
    fi
  fi
else
  echo $$ > $PIDFILE
  if [ $? -ne 0 ]
  then
    echo "Could not create PID file"
    exit 1
  fi
fi

# Base name for VG and LV.
# VG = vg_$NAME
# LV = lv_$NAME
NAME="{{vg_name}}_lvm"

# Orgtype
# Describes the orgtype (APT,DPN) for resource tags
ORGTYPE="{{orgtype|default('apt')}}"

# Environment
# Describes the environment (prod,demo,dev,test)
ENVIRONMENT="{{envtype|default('demo')}}"

# Volume Type
# Choose between Provisioned IOPS SSD (io1), General Purpose SSD (gp2),
# Throughput Optimized HDD (st1), Cold HDD (sc1)
VOLUME_TYPE="{{volume_type|default('gp2')}}"

# Where to mount it
MOUNT_POINT="{{vg_mount_point}}"

# Size of each disk in GB
# Maximum EBS size is 1024GB
DISK_SIZE="{{vg_disk_size}}"
# %age of free space (relative to disk size) before adding a new disk
SPACE_UP="{{vg_space_up}}"

# %age of free space above 1 free disk (relative to disk size) before removing one.
SPACE_DOWN="{{vg_space_down}}"

# If you want to start with a higher disk identifier to leave room for other partitions.
# /dev/sda is the root device
# /dev/sdb is the default instance-store partition
# No disk above /dev/sdz will be created

START_DISK_LETTER='{{vg_start_disk_letter}}'
# Minimum number of disks to keep
MIN_DISKS="{{vg_min_disks}}"

# Maximum number of disks to use
MAX_DISKS="{{vg_max_disks}}"

# base AWS CLI command
AWS_EC2="/usr/local/bin/aws ec2"

# NSQ Address
# e.g. demo-services.aptrust.org:4151
NSQ_ADDRESS="{{nsq_address}}"

# Pharos API creds to check running processes before shrinking
PHAROS_URL="{{pharos_url}}"
PHAROS_API_USER="{{pharos_api_key}}"
PHAROS_API_KEY="{{pharos_api_user}}"

# Notification type 'slack' or 'email'
NOTIFICATION_TYPE='slack'

# Slack settings
EBS_SLACK_WEBHOOK="{{ebs_slack_webhook}}"
EBS_SLACK_CHANNEL="{{ebs_slack_channel}}"
EBS_SLACK_USERNAME=${EBS_SLACK_USERNAME:-$(hostname | cut --delimiter=. --fields=1)}
EBS_SLACK_ICON_EMOJI=${EBS_SLACK_ICON_EMOJI:-:slack:}

function send_msg {
    if [[ $NOTIFICATION_TYPE == 'slack' ]];then
      send_slack $@
    else
      send_email $@
    fi
}


function log {
    echo "[$(date --rfc-3339=seconds)]: $*"
}

send_slack() {

    EBS_SLACK_MESSAGE=$*
		curl --silent --data-urlencode \
			"$(printf 'payload={"text": "%s", "channel": "%s", "username": "%s",
            "as_user": "true", "link_names": "true", "icon_emoji": "%s" }' \
					"${EBS_SLACK_MESSAGE}" \
					"${EBS_SLACK_CHANNEL}" \
					"${EBS_SLACK_USERNAME}" \
					"${EBS_SLACK_ICON_EMOJI}" \
			)"	${EBS_SLACK_WEBHOOK}

}

send_email() {
		SENDER=$(whoami)
		RECEIVER="christian@aptrust.org"
		USER="noreply"

		SUBJECT=$1
		BODY=${2:-''}

		MAIL_TXT="Subject: $SUBJECT\nFrom: $SENDER\nTo: $RECEIVER\n\n$BODY"
		echo -e $MAIL_TXT | sendmail -t
}


chr() {
  # converts decimal value to its ASCII character representation
  printf \\$(printf '%03o' $1)
}
ord() {
  # converts ASCII character to its decimal value
  printf '%d' "'$1"
}


check_if_mounted(){
    log "Function: ${FUNCNAME[0]}"
    if grep -qs "${MOUNT_POINT}" /proc/mounts; then
        log "${MOUNT_POINT} is mounted. All good."
        add_to_fstab
    else
        log "${MOUNT_POINT} not mounted. Go panic."
        send_msg "${MOUNT_POINT} not mounted. Panic at the disco."
        exit 1
    fi
}

add_to_fstab(){
    log ${FUNCNAME[0]}
    fstab=/etc/fstab
    # Workaround to get the device name since Ubuntu thinks its funny to double
    # dash the vg.
    MOUNT_DEVICE=$(grep $MOUNT_POINT /proc/mounts | awk '{print $1}')
    [ -z "$MOUNT_DEVICE" ] && { echo "No mounts. Empty. exit here." && exit 1; }

    fstab_entry="$MOUNT_DEVICE $MOUNT_POINT ext4 rw,nofail,x-systemd.device-timeout=1 0 0"

    if grep -q "$fstab_entry" $fstab; then
         log "Entry in fstab exists. Just made sure it is there."
    else
        log "Entry in fstab does not exists. Adding for persistence."
        sudo cp $fstab /etc/fstab.bak
        sudo bash -c "echo $fstab_entry >> /etc/fstab"
        if sudo mount -fav; then
          log "Checked fstab, looks good."
        else
          log "Error in fstab entry. Need manual intervention.\
                Reverting to previous version."
          sudo cp /etc/fstab.bak $fstab
          send_msg "fstab entry has error on $HOSTNAME"
          exit 1
        fi
    fi
  }

next_disk() {
    log ${FUNCNAME[0]}
    # No disk exist yet
    if [ "x"$1 == "x" ]; then
        DISK_LETTER=$START_DISK_LETTER
    else
      num=$(ord $1)
# TODO: Allow for second-level disk letters, https://www.pivotaltracker.com/story/show/154039718
        if [ $num -ge 122 ];then
            # Too lazy to handle /dev/sdaa, 24 disks (24TB) should be enough, no?
            log "No more disk letter available"
            send_msg "Ran out of disk letters.  Need manual intervention"
            DISK_LETTER=""
            return 1
        fi
        let num=$num+1
        DISK_LETTER=$(chr $num)
    fi
}

add_disk() {
    log "${FUNCNAME[0]}"
    log "---EXPANDING LVM START----"
    last_disk
    next_disk $LAST_DISK_LETTER || return 1

    $AWS_EC2 create-volume \
      --volume-type ${VOLUME_TYPE} \
      --size $DISK_SIZE \
      --availability-zone $AV_ZONE \
      --tag-specifications "ResourceType=volume,
                            Tags=[{Key=Name,Value=ebsscale_${NAME}_disk_${DISK_LETTER}},
                            {Key=Organisation_type,Value=${ORGTYPE}},
                            {Key=Environment,Value=${ENVIRONMENT}}]" > /etc/ebs_volume_info
    volume_id=$(jq -r '.VolumeId' /etc/ebs_volume_info)

    volume_status=""
    while [ "x"$volume_status != "xavailable" ]; do
        sleep 1
        $AWS_EC2 describe-volumes --volume-ids $volume_id > /etc/ebs_volume_info
        volume_status=$(jq -r '.Volumes[].State' /etc/ebs_volume_info)
    done

    log "Attaching disk letter: $DISK_LETTER"

    $AWS_EC2 attach-volume --volume-id $volume_id --instance-id $INSTANCE_ID --device /dev/sd${DISK_LETTER} || return 1
    attached=""
    while [ "x"$attached != "xattached" ]; do
        sleep 1
        $AWS_EC2 describe-volumes --volume-ids $volume_id > /etc/ebs_volume_info
        $AWS_EC2 describe-instances --instance-ids $INSTANCE_ID > /etc/ec2_instance_info
        attached=$(jq --arg drive "/dev/sd${DISK_LETTER}" -r '.Reservations[].Instances[].BlockDeviceMappings[] | {name: .DeviceName, status: .Ebs.Status}|select(.name==$drive)|.status' /etc/ec2_instance_info)
    done
    $AWS_EC2 modify-instance-attribute --instance-id $INSTANCE_ID --block-device-mappings "[{\"DeviceName\": \"/dev/sd${DISK_LETTER}\",\"Ebs\":{\"DeleteOnTermination\":true}}]" || return 1

    # https://www.logicworks.com/blog/2018/03/manage-aws-ebs-volumes-c5-m5-puppet-chef-ansible/
    stat /dev/xvd${DISK_LETTER} || { log "Device does not exist." && exit 1; }

    pvcreation=$(sudo pvcreate /dev/xvd${DISK_LETTER} || return 1) #/dev/sdX is attached as /dev/xvdX
    log "$pvcreation"
    add_to_fstab
    send_msg "EBS Scale /dev/xvd${DISK_LETTER} on server $HOSTNAME" \
             "EBS scale extended LVM drive by $DISK_SIZE"
}

last_disk() {
    log "Function: ${FUNCNAME[0]}"
    $AWS_EC2 describe-instances --instance-ids $INSTANCE_ID > /etc/ec2_instance_info
    root_device=$(jq -r '.Reservations[].Instances[].RootDeviceName' /etc/ec2_instance_info)
    LAST_DISK_LETTER=$(jq -r '.Reservations[].Instances[].BlockDeviceMappings[].DeviceName' /etc/ec2_instance_info | grep -v $root_device | sort | tail -n1 | sed -e 's/^.*\([a-z]\)$/\1/')
}

initialize_lv() {
    log "${FUNCNAME[0]}"
    $AWS_EC2 describe-instances --instance-ids $INSTANCE_ID > /etc/ec2_instance_info
    sudo vgcreate vg_$NAME /dev/xvd${DISK_LETTER}
    sudo lvcreate -l 100%VG -n lv_$NAME vg_$NAME
    sudo mkfs.ext4 /dev/vg_$NAME/lv_$NAME

    sudo mkdir -p $MOUNT_POINT
    sudo mount /dev/vg_$NAME/lv_$NAME $MOUNT_POINT

    add_to_fstab

}

extend_lv() {
    log "${FUNCNAME[0]}"
    sudo vgextend vg_$NAME /dev/xvd${DISK_LETTER}
    sudo lvextend -l +100%FREE /dev/vg_$NAME/lv_$NAME
    sudo resize2fs /dev/vg_$NAME/lv_$NAME
    TOTAL_NEW_SIZE=$(df | grep $MOUNT_POINT | awk '{print $2}')
    send_msg "New LVM size: $TOTAL_NEW_SIZE \n $(df -h $MOUNT_POINT)"
    log "---EXPANDING LVM END----"
}

INSTANCE_ID=$(wget -q -O - http://169.254.169.254/latest/meta-data/instance-id)
AV_ZONE=$(wget -q -O - http://169.254.169.254/latest/meta-data/placement/availability-zone)
REGION="$(echo \"$AV_ZONE\" | sed -e 's:\([0-9][0-9]*\)[a-z]*\$:\\1:')"

# Create base VG if not existing
if [[ ! -d "/dev/vg_$NAME/lv_$NAME" && ! -L "/dev/vg_$NAME/lv_$NAME" ]]; then
    if [ ! $(which unzip) ];then
        apt-get -y update
        apt-get -y install unzip lvm2 jq
    fi
    if [ ! -f /usr/local/bin/aws ]; then
        wget -O awscli-bundle.zip https://s3.amazonaws.com/aws-cli/awscli-bundle.zip
        unzip -u awscli-bundle.zip
        ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws
        mkdir -p ~/.aws
        # If we want to hardcode the credentials in the config:
        # cat > ~/.aws/config << EOF
# [default]
# aws_access_key_id = MY_ACCESS_KEY
# aws_secret_access_key = MY_SECRET
# region = $REGION
# EOF

        cat > ~/.aws/config << EOF
[default]
region = $REGION
EOF
    fi
# Main function
    add_disk || exit 1
    initialize_lv || exit 1
    for n in $(seq 2 $MIN_DISKS);do
        add_disk || exit 1
        extend_lv || exit 1
    done
fi

# First check if mounted. If it's not all following functions error out.
check_if_mounted

# Checking if we need to do something
free_space=$(df $MOUNT_POINT | sed 1d | awk '{print $4}')

# Get LVM free size to calc threshold values correctly
LVM_SIZE=$(sudo vgs --units g --nosuffix |grep $NAME |awk '{print $6}')

# Calc threshold values
min_free_space=$(echo "($SPACE_UP/100*$DISK_SIZE)*1000000" | bc -l)
# Needs 1.x (that's why +100) times a disk of free space. This may be doubled with min_disk_needed
max_free_space=$(echo "(($SPACE_DOWN+100)/100*$DISK_SIZE)*1000000" | bc -l)
#max_free_space=`echo "(($SPACE_DOWN)/100*$LVM_SIZE)*1000000" | bc -l`

# Check if more space or less space is needed
more_space_needed=$(echo $free_space'<'$min_free_space | bc -l)
less_space_needed=$(echo $free_space'>'$max_free_space | bc -l)

# DEBUG
log "lvm_size: $LVM_SIZE"
log "free_space: $free_space"
log "min_free_space: $min_free_space"
log "max_free_space: $max_free_space"
log "more_space_needed: $more_space_needed"
log "less_space_needed: $less_space_needed"

# Amount of mounted disks
actual_disks=$(sudo lvdisplay /dev/vg_${NAME}/lv_${NAME}|grep Segments|awk '{print $2}')

# Check if we are inside disk limit bounds
max_limit_reached=$(echo $actual_disks'>='$MAX_DISKS | bc -l)
min_limit_reached=$(echo $actual_disks'<='$MIN_DISKS | bc -l)

# When more space needed, add and extend as long we are insinde the disk amount limit
if [[ $more_space_needed -eq 1 ]]; then
    if [[ $max_limit_reached -eq 1 ]]; then
        log "Space needed but maximum disk limit reached"
        send_msg "Space needed but maximum disk limit reached. Please advise. \n
                    df -h $MOUNT_POINT"
    else
        log "More disk space needed."
        add_disk || exit 1
        extend_lv || exit 1
    fi
else
  diskusage=$(df |grep $MOUNT_POINT|awk '{print $5}')
  log "Scale up: No additional disks needed. Disk usage at $diskusage within limit of $((100-$SPACE_UP))%."
fi

check_nsq_inflight(){
    # If NSQ is not used it will default to empty and will default to the else clause below.
        # Added timeout to fail fast if no NSQ is used. It'll default to else clause.
    # Disabled querying the API for now since long running exchange-service tasks
    # may hit timeout in NSQ and won't show up there anymore. This leads to
    # premature shutdown of services and interrupting running processes.
    # airtraffic=$(timeout 3 curl http://$NSQ_ADDRESS/stats | grep channel|awk '{ print $7,$8; }'| grep -o '[0-9]'|tr -d "\n")
    airtraffic=$(curl -H "Content-Type: application/json" -H "Accept: application/json" -H "X-Pharos-API-User: $PHAROS_API_USER" -H "X-Pharos-API-Key: $PHAROS_API_KEY" -G "$PHAROS_URL/api/v2/items?node_not_empty=true&pid_not_empty=true" | jq .count)

# Check for Rsync jobs on when script is run on DPN node.
    rsyncing=$(ps axu |pgrep rsync | wc -l)

     if [[ "$airtraffic" -gt "0" || "$rsyncing" -gt "0" ]]; then
       log "Seeing some air traffic: $airtraffic and/or rsyncs $rsyncing. Please stand by. "
       nsq_result="standby"
     else
       log "Nobody is flying. Good to go."
       nsq_result="takeoff"
     fi
}

# When less space needed and still inside the disk limit print out manual action to carry out.
# resize2fs can't do a resize while the parition is mounted. The partition needs to be unmounted first,
# which mostly requires the service to be put in maintencance mode

if [[ $less_space_needed -eq 1 ]]; then
    if [[ $min_limit_reached -eq 1 ]]; then
        log "Too much space but minimum disk limit reached"
    else
        # In case the DISK_SIZE has changed since creating the LVM.
        LAST_DISK_SIZE=$(sudo pvs --units g|tail -n1| awk '{print $5}'| sed 's/.$//')
        root_device_name=$(jq -r '.Reservations[].Instances[].RootDeviceName' /etc/ec2_instance_info)
        LAST_VOLUME=$(jq -r '.Reservations[].Instances[].BlockDeviceMappings[] | select(.DeviceName != "'$root_device_name'") | .Ebs.VolumeId' /etc/ec2_instance_info | tail -n1)
        log "Root device info: $root_device_name \t LastVolume $LAST_VOLUME"
        # Check for inflight items to avoid stopping services if not necessary.
        # Shrinking will commence once no worker is processing items.
        check_nsq_inflight
        log "NSQ_RESULT: $nsq_result"
        if [ $nsq_result == "takeoff" ];then
            log "---SHRINKING VOLUME START----"
            send_msg "Worker queues empty. Shutting down exchange services. \n \
                $(df -h $MOUNT_POINT) Shrinking by $LAST_DISK_SIZE GB"
            log "Shutting down exchange services. \n $(df -h $MOUNT_POINT)"
            sudo supervisorctl stop all
            sudo docker stop exchange_nsqd_1 exchange_nsqlookupd_1 exchange_nsqadmin_1
            # Set dummy value to run through loop at least once
            openfiles=100
            while [ "$openfiles" -gt "0" ]; do
              log "Waiting until no open files are present"
              sleep 3
              openfiles=$(sudo lsof $MOUNT_POINT | wc -l)
              log "Open files= $openfiles"
            done
            umount $MOUNT_POINT
            if [ $? -eq 0 ]; then
              log "Unmount of VG successful. LAST_VOLUME is $LAST_VOLUME and will be removed"
              time e2fsck -f -y /dev/vg_${NAME}/lv_${NAME}
              lvreduction=$(time lvreduce -A y -f -r -L -${LAST_DISK_SIZE}G vg_${NAME}/lv_${NAME} || exit 1)
              if [ $? -eq 0 ]; then
                  log $lvreduction
                  reduct=$(vgreduce -a vg_${NAME})
                  log $reduct
                  lvextend -r -l +100%FREE /dev/vg_${NAME}/lv_${NAME} || exit 1
                  mount /dev/vg_${NAME}/lv_$NAME $MOUNT_POINT || exit 1
                  # TODO Add check if LV has been removed, if not exit 1
                  $AWS_EC2 detach-volume --instance-id $INSTANCE_ID --volume-id $LAST_VOLUME
                  volume_status=""
                  while [ "x"$volume_status != "xavailable" ]; do
                    log "Waiting for detached volume to become available..."
                    sleep 2
                    $AWS_EC2 describe-volumes --volume-ids $LAST_VOLUME > /etc/ebs_to_shrink
                    volume_status=$(jq -r '.Volumes[].State' /etc/ebs_to_shrink)
                  done
                  log "Volume detached and available. Deleting Volume $LAST_VOLUME"
                  $AWS_EC2 delete-volume --volume-id $LAST_VOLUME
                  # Update ec2_instance_info to have the currrently attached volumes up-to-date.
                  $AWS_EC2 describe-instances --instance-ids $INSTANCE_ID > /etc/ec2_instance_info
                  log "LVM shrinking completed. Re-mounting LVM & restarting exchange services."
              # Note: already mounted on line 414
              #    sudo mount $MOUNT_POINT
                  sudo supervisorctl start exchange:
                  sudo docker start exchange_nsqd_1 exchange_nsqlookupd_1 exchange_nsqadmin_1
                  send_msg "LVM shrinking completed. New drive: \n $(df -h $MOUNT_POINT)"
                  log "---SHRINKING VOLUME END----"
              fi
            else
              send_msg "Unable to unmount. Aborting shrinking on $hostname"
              sudo supervisorctl start exchange:
              sudo docker start exchange_nsqd_1 exchange_nsqlookupd_1 exchange_nsqadmin_1
              exit 1
            fi
        else
            log "Can't shrink right now, trying again later"
        fi
    fi
else
    diskusage=$(df |grep $MOUNT_POINT|awk '{print $5}' | sed 's/%//')
    log "Scale down: No shrinking needed. Free LVM space $((100-$diskusage))% within the limit of $((100+$SPACE_DOWN))% individual disk or minimum disks reached."
fi
